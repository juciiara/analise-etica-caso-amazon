# ğŸŒ ExperiÃªncia PrÃ¡tica â€“ Ã‰tica, Cidadania Digital e Direitos

## 1ï¸âƒ£ Escolha do Caso  
O sistema de recrutamento da **Amazon** utilizava inteligÃªncia artificial para automatizar a triagem de currÃ­culos. No entanto, acabou sendo descontinuado por **favorecer candidatos homens** e **prejudicar mulheres**, revelando um profundo **viÃ©s algorÃ­tmico**.  

Este caso mostra que tecnologias avanÃ§adas podem **reproduzir desigualdades sociais**, mesmo quando a intenÃ§Ã£o Ã© agilizar processos.

---

## 2ï¸âƒ£ AnÃ¡lise Ã‰tica Estruturada  

### âš–ï¸ 2.1 ViÃ©s e JustiÃ§a  
- **Dados histÃ³ricos refletem preconceitos culturais e institucionais**.  
- A IA **replicou padrÃµes discriminatÃ³rios**, rejeitando currÃ­culos femininos com qualificaÃ§Ãµes equivalentes Ã s masculinas.  
- **JustiÃ§a algorÃ­tmica nÃ£o Ã© automÃ¡tica**: sem supervisÃ£o, sistemas de IA podem reforÃ§ar desigualdades existentes.

### ğŸ” 2.2 TransparÃªncia e Explicabilidade  
- Funcionava como uma **â€œcaixa-pretaâ€**, sem explicaÃ§Ãµes claras para rejeiÃ§Ãµes.  
- Isso dificultou que candidatos e gestores **entendessem ou contestassem decisÃµes**, criando uma lacuna de confianÃ§a.  
- Sistemas que impactam carreiras exigem **transparÃªncia ativa**, com critÃ©rios claros e auditÃ¡veis.

### ğŸŒ 2.3 Impacto Social e Direitos  
- Excluiu estruturalmente mulheres no mercado de tecnologia.  
- Falta de transparÃªncia sobre dados pessoais levanta questÃµes legais (**LGPD**).  
- Demonstra que **responsabilidade social e conformidade legal** estÃ£o interligadas.

### ğŸ›¡ï¸ 2.4 Responsabilidade e GovernanÃ§a  
- Destaca a necessidade de **supervisÃ£o Ã©tica contÃ­nua**.  
- Medidas que poderiam ter evitado o problema:  
  - RevisÃ£o e diversificaÃ§Ã£o da base de dados;  
  - Monitoramento constante do algoritmo;  
  - InclusÃ£o de especialistas em Ã©tica e diversidade no desenvolvimento.

ğŸ“Œ **Aprendizado central**: IA sem governanÃ§a Ã©tica Ã© risco social. **DecisÃµes automatizadas nÃ£o eliminam a responsabilidade humana**.

---

## 3ï¸âƒ£ Posicionamento Final  
A descontinuaÃ§Ã£o do sistema foi necessÃ¡ria, mas o mais importante Ã© transformar o caso em **aprendizado**.  

Futuras soluÃ§Ãµes de IA devem ser planejadas com **Ã©tica desde o inÃ­cio**, garantindo que a tecnologia **corrija desigualdades, e nÃ£o as reproduza**.

---

## 4ï¸âƒ£ RecomendaÃ§Ãµes PrÃ¡ticas  
1. Implementar **auditorias contÃ­nuas de viÃ©s** e avaliaÃ§Ãµes de impacto social.  
2. Garantir **transparÃªncia e explicabilidade**, fornecendo detalhes sobre decisÃµes automatizadas.  
3. Formar **comitÃªs multidisciplinares** (tecnologia, Ã©tica e diversidade) para supervisÃ£o.  
4. Utilizar **bases de dados representativas**, evitando estereÃ³tipos histÃ³ricos.  
5. Desenvolver programas de **conscientizaÃ§Ã£o e treinamento em Ã©tica em IA**.

---

## 5ï¸âƒ£ Equipe  
- **Leandro Cavalcante**  
- **Ana Paula Bastos**  
- **Suzana Marks**  
- **Juciara E. ConceiÃ§Ã£o**  

---

ğŸ“„ [Visualizar PDF completo do relatÃ³rio](https://github.com/leandrotottioficialcantor-cpu/Educa_livre/blob/main/docs/Relatorio_Etica_Amazon.pdf)  

